{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Machine Learning Systems with Python - Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is supporting material for the book `Building Machine Learning Systems with Python` by [Willi Richert](https://www.linkedin.com/in/willirichert/) and [Luis Pedro Coelho](https://www.linkedin.com/in/luispedrocoelho/)  published by PACKT Publishing.\n",
    "\n",
    "It is made available under the MIT License.\n",
    "\n",
    "All code examples use Python in version..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.3 |Anaconda custom (64-bit)| (default, Nov  8 2017, 15:10:56) [MSC v.1900 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use all screen real-estate\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the data\n",
    "In this chapter we will use the StackOverflow data from https://archive.org/download/stackexchange (while downloading, you have a couple hours time to contemplate whether now would be a good time to donate to the awesome archive.org :-) )\n",
    "\n",
    "Since it is updated on a regular basis, you might get slightly different numbers. In this chapter we use this version:\n",
    "```\n",
    "stackoverflow.com-Posts.7z                        08-Dec-2017 22:31     11.3G\n",
    "```\n",
    "After downloading it, you need to unzip it with [7-Zip](http://www.7-zip.de/download.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting and filtering it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original posts file: F:\\Stack Exchange Data Dump - Dec 2017\\posts.xml\n",
      "Restricting to 2012: F:\\Stack Exchange Data Dump - Dec 2017\\posts-2012.xml\n",
      "Filtered: F:\\Stack Exchange Data Dump - Dec 2017\\filtered.tsv\n",
      "Filtered meta: F:\\Stack Exchange Data Dump - Dec 2017\\filtered-meta.json\n",
      "Chosen posts: C:\\repo\\ML_Book\\BuildingMachineLearningSystemsWithPython\\ch04_3rd\\10k\\data\\chosen.tsv\n",
      "Chosen meta: C:\\repo\\ML_Book\\BuildingMachineLearningSystemsWithPython\\ch04_3rd\\10k\\data\\chosen-meta.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm_notebook as tqdm # we all love nice progress bars, don't we?\n",
    "try:\n",
    "    import ujson as json  # UltraJSON if available\n",
    "except:\n",
    "    print(\"You can also use the normal json module, but you get a XXX speedup if you use ujson instead.\")\n",
    "    raise\n",
    "    \n",
    "# TODO change before merging to master\n",
    "#DATA_DIR = \"data\"  # put your posts-2012.xml into this directory\n",
    "DATA_DIR = r'F:\\Stack Exchange Data Dump - Dec 2017'\n",
    "\n",
    "YEAR = 2012\n",
    "\n",
    "fn_posts_all = os.path.join(DATA_DIR, \"posts.xml\")\n",
    "print(\"Original posts file: %s\" % fn_posts_all)\n",
    "fn_posts = os.path.join(DATA_DIR, \"posts-%i.xml\" % YEAR)\n",
    "print(\"Restricting to %i: %s\" % (YEAR, fn_posts))\n",
    "\n",
    "fn_filtered = os.path.join(DATA_DIR, \"filtered.tsv\")\n",
    "print(\"Filtered: %s\" % fn_filtered)\n",
    "fn_filtered_meta = os.path.join(DATA_DIR, \"filtered-meta.json\")\n",
    "print(\"Filtered meta: %s\" % fn_filtered_meta)\n",
    "\n",
    "CHOSEN_DIR = '10k'\n",
    "if not os.path.exists(CHOSEN_DIR):\n",
    "    os.mkdir(CHOSEN_DIR)\n",
    "\n",
    "if not os.path.exists(os.path.join(CHOSEN_DIR, 'data')):\n",
    "    os.mkdir(os.path.join(CHOSEN_DIR, 'data'))\n",
    "\n",
    "fn_chosen = os.path.abspath(os.path.join('10k', 'data', \"chosen.tsv\"))\n",
    "fn_chosen_meta = os.path.abspath(os.path.join('10k', 'data', \"chosen-meta.json\"))\n",
    "print(\"Chosen posts: %s\" % fn_chosen)\n",
    "print(\"Chosen meta: %s\" % fn_chosen_meta)\n",
    "\n",
    "CHART_DIR = os.path.join(CHOSEN_DIR, \"charts\")\n",
    "if not os.path.exists(CHART_DIR):\n",
    "    os.mkdir(CHART_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 59GB in posts.xml is contain posts from 2008 to 2017. We will use only the posts from the year 2012, which provides enough fun for now. We could simply grep on the command line, but that would take quite a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_match = re.compile(r'^\\s+<row [^>]*CreationDate=\"(\\d+)-')\n",
    "size = os.path.getsize(fn_posts_all)\n",
    "\n",
    "def get_year(line):\n",
    "    m = year_match.match(line)\n",
    "    if m is None:\n",
    "        import pdb;pdb.set_trace()\n",
    "    return int(m.group(1))\n",
    "\n",
    "\n",
    "with open(fn_posts_all, 'r', encoding='utf-8') as fa, open(fn_posts, 'w', encoding='utf-8') as f2012:\n",
    "    # first two lines are the xml header and <posts> tag\n",
    "    f2012.write('<?xml version=\"1.0\" encoding=\"utf-8\"?><posts>\\n')    \n",
    "    \n",
    "    right = size//2\n",
    "    delta = right\n",
    "    \n",
    "    # first find some post of YEAR\n",
    "    while True:\n",
    "        fa.seek(right)\n",
    "        fa.readline() # go to next newline\n",
    "        line = fa.readline()\n",
    "        \n",
    "        year = get_year(line)\n",
    "        \n",
    "        delta //= 2\n",
    "        assert delta > 0\n",
    "        \n",
    "        if year>YEAR:\n",
    "            right -= delta\n",
    "        elif year<YEAR:\n",
    "            right += delta\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    # then find where it starts\n",
    "    left = right//2\n",
    "    delta = left\n",
    "    while True:\n",
    "        fa.seek(left)\n",
    "        fa.readline() # go to next newline\n",
    "        line = fa.readline()\n",
    "        \n",
    "        year = get_year(line)\n",
    "        \n",
    "        delta //= 2\n",
    "        if delta == 0:\n",
    "            break\n",
    "        \n",
    "        if year<YEAR:\n",
    "            left += delta\n",
    "            \n",
    "        else:\n",
    "            left, right = left-delta, left\n",
    "    \n",
    "    # and write all posts of that year\n",
    "    while True:\n",
    "        line = fa.readline()\n",
    "        year = get_year(line)\n",
    "        if year == YEAR:\n",
    "            f2012.write(line)\n",
    "        elif year > YEAR:\n",
    "            break\n",
    "        \n",
    "    # and write the closing tag\n",
    "    f2012.write('</posts>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser as dateparser\n",
    "\n",
    "from operator import itemgetter\n",
    "from lxml import etree\n",
    "\n",
    "q_creation = {}  # creation datetimes of questions\n",
    "q_accepted = {}  # id of accepted answer\n",
    "\n",
    "NUM_ROWS = 4511696 # counted by hand\n",
    "\n",
    "filtered_meta = {\n",
    "    'question': defaultdict(list), # question -> [(answer Id, IsAccepted, TimeToAnswer, Score), ...]\n",
    "    'total': 0 # questions and answers finally written\n",
    "}\n",
    "\n",
    "# Regular expressions to find code snippets, links, images, and tags, which might help in \n",
    "# designing useful features\n",
    "code_match = re.compile('<pre>(.*?)</pre>', re.MULTILINE | re.DOTALL)\n",
    "link_match = re.compile('<a href=\"http://.*?\".*?>(.*?)</a>', re.MULTILINE | re.DOTALL)\n",
    "img_match = re.compile('<img(.*?)/>', re.MULTILINE | re.DOTALL)\n",
    "tag_match = re.compile('<[^>]*>', re.MULTILINE | re.DOTALL)\n",
    "whitespace_match = re.compile(r'\\s+', re.MULTILINE | re.DOTALL)\n",
    "\n",
    "def extract_features_from_body(s):\n",
    "    '''\n",
    "    This method creates features from the raw post. It already contains all \n",
    "    features that we will use throughout the chapter.\n",
    "    '''\n",
    "    num_code_lines = 0\n",
    "    link_count_in_code = 0\n",
    "    code_free_s = s\n",
    "\n",
    "    num_images = len(img_match.findall(s))\n",
    "\n",
    "    # remove source code and count how many lines\n",
    "    for match_str in code_match.findall(s):\n",
    "        num_code_lines += match_str.count('\\n')\n",
    "        code_free_s = code_match.sub(' ', code_free_s)\n",
    "\n",
    "        # sometimes source code contain links, which we don't want to count\n",
    "        link_count_in_code += len(link_match.findall(match_str))\n",
    "\n",
    "    links = link_match.findall(s)\n",
    "    link_count = len(links) - link_count_in_code\n",
    "\n",
    "    html_free_s = tag_match.sub(' ', code_free_s)\n",
    "    link_free_s = html_free_s\n",
    "    \n",
    "    for link in links:\n",
    "        if link.lower().startswith('http://'):\n",
    "            link_free_s = link_free_s.replace(link, ' ')\n",
    "\n",
    "    whitespace_cleaned_s = whitespace_match.sub(' ', link_free_s)\n",
    "    num_text_tokens = whitespace_cleaned_s.count(' ')\n",
    "\n",
    "    return link_free_s, num_text_tokens, num_code_lines, link_count, num_images\n",
    "\n",
    "years = defaultdict(int)\n",
    "num_questions = 0\n",
    "num_answers = 0\n",
    "\n",
    "def parsexml(fn):\n",
    "    global num_questions, num_answers\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    # etree.iterparse() returns a tuple (event, element). Since we request only\n",
    "    # 'start' events, we pipe the result through an itemgetter that always returns\n",
    "    # the 2nd result.\n",
    "    it = map(itemgetter(1), etree.iterparse(fn, events=('start',)))\n",
    "    \n",
    "    # Get the <posts> element, in which we will parse the <row> elements. While doing so,\n",
    "    # we will need the root handle to clear memory\n",
    "    root = next(it)\n",
    "    \n",
    "    for counter, elem in enumerate(tqdm(it, total=NUM_ROWS)):\n",
    "        \n",
    "        if elem.tag != 'row':\n",
    "            continue\n",
    "            \n",
    "        creation_date = dateparser.parse(elem.get('CreationDate'))\n",
    "\n",
    "        Id = int(elem.get('Id'))\n",
    "        PostTypeId = int(elem.get('PostTypeId'))\n",
    "        Score = int(elem.get('Score'))\n",
    "\n",
    "        if PostTypeId == 1:\n",
    "            num_questions += 1\n",
    "            years[creation_date.year] += 1\n",
    "\n",
    "            ParentId = -1\n",
    "            TimeToAnswer = 0\n",
    "            q_creation[Id] = creation_date\n",
    "            accepted = elem.get('AcceptedAnswerId')\n",
    "            if accepted:\n",
    "                q_accepted[Id] = int(accepted)\n",
    "            IsAccepted = 0\n",
    "\n",
    "        elif PostTypeId == 2:\n",
    "            num_answers += 1\n",
    "\n",
    "            ParentId = int(elem.get('ParentId'))\n",
    "            if not ParentId in q_creation:\n",
    "                # question is older than 2012\n",
    "                continue\n",
    "\n",
    "            TimeToAnswer = (creation_date - q_creation[ParentId]).seconds\n",
    "\n",
    "            if ParentId in q_accepted:\n",
    "                IsAccepted = int(q_accepted[ParentId] == Id)\n",
    "            else:\n",
    "                IsAccepted = 0\n",
    "\n",
    "            filtered_meta['question'][ParentId].append((Id, IsAccepted, TimeToAnswer, Score))\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        Text, NumTextTokens, NumCodeLines, LinkCount, NumImages = extract_features_from_body(elem.get('Body'))\n",
    "\n",
    "        # https://www.ibm.com/developerworks/xml/library/x-hiperfparse/\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "            \n",
    "        values = (Id, ParentId,\n",
    "                  IsAccepted,\n",
    "                  TimeToAnswer, Score,\n",
    "                  Text,\n",
    "                  NumTextTokens, NumCodeLines, LinkCount, NumImages)\n",
    "\n",
    "        yield values\n",
    "\n",
    "\n",
    "    print(\"Found %i posts\" % counter)\n",
    "\n",
    "if any(not os.path.exists(fn) for fn in [fn_filtered, fn_filtered_meta]):\n",
    "    total = 0\n",
    "    with open(fn_filtered, \"w\") as f:\n",
    "        for values in parsexml(fn_posts):\n",
    "            line = \"\\t\".join(map(str, values))\n",
    "            f.write(line + \"\\n\")\n",
    "            total += 1\n",
    "    filtered_meta['total'] = total\n",
    "                \n",
    "    with open(fn_filtered_meta, \"w\") as f:\n",
    "        json.dump(filtered_meta, f)\n",
    "    \n",
    "    print(\"years:\", years)\n",
    "    print(\"#qestions: %i\" % num_questions)\n",
    "    print(\"#answers: %i\" % num_answers)\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping the conversion step, loading data from %s ...\" % fn_filtered_meta)\n",
    "    filtered_meta = json.load(open(fn_filtered_meta, \"r\"))\n",
    "    print(\"... done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to select the answers that we want to keep per question. We do this in two stages:\n",
    " * Stage 1: Select which answers to keep per question (`filter_method` lets you chose among different methods)\n",
    " * Stage 2: Filter the previously stored features according to the answers selected in stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    filtered_meta = json.load(open(r\"F:\\pymlbook-data\\filtered-meta.json\", \"r\"))\n",
    "    filtered_meta['question'] = filtered_meta\n",
    "    fn_filtered = r\"F:\\pymlbook-data\\filtered.tsv\"\n",
    "    NUM_ROWS = 4511696 # counted by hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In the book chapter we will use \"negative_positive\".\n",
    "#\n",
    "# \"negative_positive\":   keep the best and worst, but only if we have one with \n",
    "#                        positive and one with negative score\n",
    "# \"only_one_per_class\":  only keep the lowest scoring answer per class in addition to the \n",
    "#                        accepted one\n",
    "# \"sample_per_question\": if not None, specifies the number of unaccepted per question\n",
    "# \"half-half\":           equal share of questions that are unanswered and those that are \n",
    "#                        answered\n",
    "\n",
    "filter_method = \"negative_positive\"\n",
    "\n",
    "MAX_ANSWERS_PER_QUESTIONS = 10  # used by filter_method \"sample_per_question\"\n",
    "\n",
    "NUM_QUESTION_SAMPLE = 10000\n",
    "\n",
    "posts_to_keep = set()\n",
    "found_questions = 0\n",
    "\n",
    "unaccepted_scores = {}\n",
    "\n",
    "has_q_accepted_a = {}\n",
    "num_q_with_accepted_a = 0\n",
    "num_q_without_accepted_a = 0\n",
    "\n",
    "question = filtered_meta['question']\n",
    "for ParentId, posts in tqdm(question.items(), total=len(question), desc=\"Stage 1:\"):\n",
    "    assert ParentId != -1\n",
    "\n",
    "    if len(posts) < 2:\n",
    "        continue\n",
    "\n",
    "    ParentId = int(ParentId)\n",
    "    AllIds = set([ParentId])\n",
    "    AcceptedId = None\n",
    "    UnacceptedId = None\n",
    "    UnacceptedIds = []\n",
    "    UnacceptedScore = sys.maxsize\n",
    "\n",
    "    NegativeScoreIds = []\n",
    "    PositiveScoreIds = []\n",
    "\n",
    "    if filter_method == \"half-half\":\n",
    "\n",
    "        has_accepted_a = False\n",
    "        for post in posts:\n",
    "            Id, IsAccepted, TimeToAnswer, Score = post\n",
    "\n",
    "            if IsAccepted:\n",
    "                has_accepted_a = True\n",
    "                break\n",
    "\n",
    "        has_q_accepted_a[ParentId] = has_accepted_a\n",
    "\n",
    "        if has_accepted_a:\n",
    "            if num_q_with_accepted_a < NUM_QUESTION_SAMPLE / 2:\n",
    "                num_q_with_accepted_a += 1\n",
    "                posts_to_keep.add(ParentId)\n",
    "        else:\n",
    "            if num_q_without_accepted_a < NUM_QUESTION_SAMPLE / 2:\n",
    "                num_q_without_accepted_a += 1\n",
    "                posts_to_keep.add(ParentId)\n",
    "\n",
    "        if num_q_without_accepted_a + num_q_with_accepted_a > NUM_QUESTION_SAMPLE:\n",
    "            assert -1 not in posts_to_keep\n",
    "            break\n",
    "\n",
    "    else:\n",
    "\n",
    "        for post in posts:\n",
    "            Id, IsAccepted, TimeToAnswer, Score = post\n",
    "\n",
    "            if filter_method == \"all\":\n",
    "                AllIds.add(int(Id))\n",
    "\n",
    "            elif filter_method == \"only_one_per_class\":\n",
    "                if IsAccepted:\n",
    "                    AcceptedId = Id\n",
    "                elif Score < UnacceptedScore:\n",
    "                    UnacceptedScore = Score\n",
    "                    UnacceptedId = Id\n",
    "\n",
    "            elif filter_method == \"sample_per_question\":\n",
    "                if IsAccepted:\n",
    "                    AcceptedId = Id\n",
    "                else:\n",
    "                    UnacceptedIds.append(Id)\n",
    "\n",
    "            elif filter_method == \"negative_positive\":\n",
    "                if Score <= 0:\n",
    "                    NegativeScoreIds.append((Score, Id))\n",
    "                elif Score > 0:\n",
    "                    PositiveScoreIds.append((Score, Id))\n",
    "\n",
    "            else:\n",
    "                raise ValueError(filter_method)\n",
    "\n",
    "        added = False\n",
    "        if filter_method == \"all\":\n",
    "            posts_to_keep.update(AllIds)\n",
    "            added = True\n",
    "            \n",
    "        elif filter_method == \"only_one_per_class\":\n",
    "            if AcceptedId is not None and UnacceptedId is not None:\n",
    "                posts_to_keep.add(ParentId)\n",
    "                posts_to_keep.add(AcceptedId)\n",
    "                posts_to_keep.add(UnacceptedId)\n",
    "                added = True\n",
    "\n",
    "        elif filter_method == \"sample_per_question\":\n",
    "            if AcceptedId is not None and UnacceptedIds is not None:\n",
    "                posts_to_keep.add(ParentId)\n",
    "                posts_to_keep.add(AcceptedId)\n",
    "                posts_to_keep.update(UnacceptedIds[:MAX_ANSWERS_PER_QUESTIONS])\n",
    "                added = True\n",
    "\n",
    "        elif filter_method == \"negative_positive\":\n",
    "            if PositiveScoreIds and NegativeScoreIds:\n",
    "                posts_to_keep.add(ParentId)\n",
    "\n",
    "                posScore, posId = sorted(PositiveScoreIds)[-1]\n",
    "                posts_to_keep.add(posId)\n",
    "\n",
    "                negScore, negId = sorted(NegativeScoreIds)[0]\n",
    "                posts_to_keep.add(negId)\n",
    "                \n",
    "                added = True\n",
    "\n",
    "        if added:\n",
    "            found_questions += 1\n",
    "\n",
    "    if NUM_QUESTION_SAMPLE and found_questions >= NUM_QUESTION_SAMPLE:\n",
    "        print(\"Using only a sample of %i questions\" % NUM_QUESTION_SAMPLE)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_written = set()\n",
    "chosen_meta = defaultdict(dict)\n",
    "\n",
    "total = 0\n",
    "kept = 0\n",
    "\n",
    "with open(fn_chosen, \"w\") as f:\n",
    "    for line in tqdm(open(fn_filtered, 'r'), total=NUM_ROWS, desc=\"Stage 2:\"):\n",
    "        strId, ParentId, IsAccepted, TimeToAnswer, Score, Text, NumTextTokens, NumCodeLines, LinkCount, NumImages = line.strip().split(\"\\t\")\n",
    "\n",
    "        Text = Text.strip()\n",
    "\n",
    "        total += 1\n",
    "\n",
    "        Id = int(strId)\n",
    "        if Id in posts_to_keep:\n",
    "            if Id in already_written:\n",
    "                print(Id, \"is already written\")\n",
    "                continue\n",
    "\n",
    "            # setting meta info\n",
    "            post = chosen_meta[Id]\n",
    "            post['ParentId'] = int(ParentId)\n",
    "            post['IsAccepted'] = int(IsAccepted)\n",
    "            post['TimeToAnswer'] = int(TimeToAnswer)\n",
    "            post['Score'] = int(Score)\n",
    "            post['NumTextTokens'] = int(NumTextTokens)\n",
    "            post['NumCodeLines'] = int(NumCodeLines)\n",
    "            post['LinkCount'] = int(LinkCount)\n",
    "            post['NumImages'] = int(NumImages)\n",
    "            post['idx'] = kept  # index into the file\n",
    "\n",
    "            if int(ParentId) == -1:\n",
    "                q = chosen_meta[Id]\n",
    "\n",
    "                if not 'Answers' in q:\n",
    "                    q['Answers'] = []\n",
    "\n",
    "                if filter_method == \"half-half\":\n",
    "                    q['HasAcceptedAnswer'] = has_q_accepted_a[Id]\n",
    "\n",
    "            else:\n",
    "                q = chosen_meta[int(ParentId)]\n",
    "\n",
    "                if int(IsAccepted) == 1:\n",
    "                    assert 'HasAcceptedAnswer' not in q\n",
    "                    q['HasAcceptedAnswer'] = True\n",
    "\n",
    "                if 'Answers' not in q:\n",
    "                    q['Answers'] = [Id]\n",
    "                else:\n",
    "                    q['Answers'].append(Id)\n",
    "\n",
    "            f.writelines(\"%s\\t%s\\n\" % (Id, Text))\n",
    "            kept += 1\n",
    "\n",
    "with open(fn_chosen_meta, \"w\") as fm:\n",
    "    json.dump(chosen_meta, fm)\n",
    "\n",
    "print(\"read:\", total)\n",
    "print(\"kept:\", kept)\n",
    "print(\"TODO wrong total in tqdm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions to access the sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta(fn):\n",
    "    meta = json.load(open(fn, \"r\"))\n",
    "    keys = list(meta.keys())\n",
    "\n",
    "    # JSON only allows string keys, changing that to int\n",
    "    for key in keys:\n",
    "        meta[int(key)] = meta[key]\n",
    "        del meta[key]\n",
    "\n",
    "    # map post Id to index\n",
    "    id_to_idx = {}\n",
    "    # and back\n",
    "    idx_to_id = {}\n",
    "\n",
    "    for PostId, Info in meta.items():\n",
    "        id_to_idx[PostId] = idx = Info['idx']\n",
    "        idx_to_id[idx] = PostId\n",
    "\n",
    "    return meta, id_to_idx, idx_to_id\n",
    "\n",
    "meta, id_to_idx, idx_to_id = load_meta(fn_chosen_meta)\n",
    "\n",
    "#meta, id_to_idx, idx_to_id = load_meta(r'F:\\pymlbook-data\\chosen-meta.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the features and labeling them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = sorted([q for q, v in meta.items() if v['ParentId'] == -1])\n",
    "all_answers = sorted([q for q, v in meta.items() if v['ParentId'] != -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_questions), len(all_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An answer is labeled as positive if it has a score greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_orig = np.asarray([meta[aid]['Score'] > 0 for aid in all_answers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating our first classifier: kNN using only LinkCount as a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors \n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=2) \n",
    "print(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy training data: map {1,2,3} to 0 and {4,5,6} to 1\n",
    "knn.fit([[1],[2],[3],[4],[5],[6]], [0,0,0,1,1,1])\n",
    "knn.predict(1.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict(37) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict_proba(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict_proba(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict_proba(3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model using only LinkCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how good is `LinkCount`? Let's look at its histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "X = np.asarray([[meta[aid]['LinkCount']] for aid in all_answers])\n",
    "\n",
    "plt.figure(figsize=(5,4), dpi=300) # width and height of the plot in inches\n",
    "\n",
    "plt.title('LinkCount')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Occurrence')\n",
    "\n",
    "n, bins, patches = plt.hist(X, normed=1, bins=range(max(X.ravel())-min(X.ravel())), alpha=0.75)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(CHART_DIR, 'feat_hist_linkcount.png'), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so most posts don't contain a link at all, but let's try nevertheless..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on LinkCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "scores = []\n",
    "N_FOLDS = 10\n",
    "from sklearn.utils import shuffle\n",
    "#X, Y = shuffle(X, Y_orig, random_state=0)\n",
    "cv = KFold(n=len(X), n_folds=N_FOLDS)\n",
    "\n",
    "for train, test in tqdm(cv, total=N_FOLDS):    \n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(X[train], Y[train])\n",
    "    scores.append(clf.score(X[test], Y[test]))\n",
    "\n",
    "print(\"Mean(scores)=%.5f\\tStddev(scores)=%.5f\"%(np.mean(scores), np.std(scores))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feat_hist(data_name_list, filename=None):\n",
    "    if len(data_name_list) > 1:\n",
    "        assert filename is not None\n",
    "\n",
    "    num_rows = int(1 + (len(data_name_list) - 1) / 2)\n",
    "    num_cols = int(1 if len(data_name_list) == 1 else 2)\n",
    "    plt.figure(figsize=(5 * num_cols, 4 * num_rows), dpi=300)\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            plt.subplot(num_rows, num_cols, 1 + i * num_cols + j)\n",
    "            x, name = data_name_list[i * num_cols + j]\n",
    "            plt.title(name)\n",
    "            plt.xlabel('Value')\n",
    "            plt.ylabel('Occurrence')\n",
    "            \n",
    "            max_val = max(x.ravel())\n",
    "            if max_val>100:\n",
    "                bins = range(0, max_val, 10)\n",
    "            else:\n",
    "                bins = range(0, max_val)\n",
    "            \n",
    "            n, bins, patches = plt.hist(x, bins=bins, normed=1, alpha=0.75)\n",
    "\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "    \n",
    "    if not filename:\n",
    "        filename = \"feat_hist_%s.png\" % name.replace(\" \", \"_\")\n",
    "\n",
    "    plt.savefig(os.path.join(CHART_DIR, filename))\n",
    "\n",
    "\n",
    "plot_feat_hist([(np.asarray([[meta[aid]['NumCodeLines']] for aid in all_answers]), 'NumCodeLines'),\n",
    "                (np.asarray([[meta[aid]['NumTextTokens']] for aid in all_answers]), 'NumTextTokens')],\n",
    "              'feat_hist_CodeLines_TextTokens.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(aid, feature_names):\n",
    "    return tuple(meta[aid][fn] for fn in feature_names)\n",
    "\n",
    "X = np.asarray([get_features(aid, ['LinkCount', 'NumCodeLines', 'NumTextTokens']) for aid in all_answers])\n",
    "\n",
    "scores = []\n",
    "from sklearn.utils import shuffle\n",
    "#X, Y = shuffle(X, Y_orig, random_state=0)\n",
    "X, Y = X, Y_orig\n",
    "cv = KFold(n=len(X), n_folds=N_FOLDS)\n",
    "\n",
    "for train, test in tqdm(cv, total=N_FOLDS):    \n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(X[train], Y[train])\n",
    "    scores.append(clf.score(X[test], Y[test]))\n",
    "\n",
    "print(\"Mean(scores)=%.5f\\tStddev(scores)=%.5f\"%(np.mean(scores), np.std(scores))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If more features are good, even more features should be even better\n",
    "Let's create some more text based features like average sentence and word length, how many words are CAPITALIZED or contain exclamation marks.\n",
    "\n",
    "We simply fetch the post texts, calculate the statistics and add them to the `meta` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def fetch_posts(fn, with_index=True, line_count=-1):\n",
    "    count = 0\n",
    "\n",
    "    for line in open(fn, \"r\"):\n",
    "        count += 1\n",
    "        if line_count > 0 and count > line_count:\n",
    "            break\n",
    "\n",
    "        Id, Text = line.split(\"\\t\")\n",
    "        Text = Text.strip()\n",
    "\n",
    "        if with_index:\n",
    "            yield int(Id), Text\n",
    "\n",
    "        else:\n",
    "            yield Text\n",
    "            \n",
    "def add_sentence_features(m):\n",
    "    for pid, text in fetch_posts(fn_chosen, with_index=True):\n",
    "        if not text:\n",
    "            m[pid]['AvgSentLen'] = m[pid]['AvgWordLen'] = m[pid]['NumAllCaps'] = m[pid]['NumExclams'] = 0\n",
    "        else:\n",
    "            sent_lens = [len(nltk.word_tokenize(sent)) for sent in nltk.sent_tokenize(text)]\n",
    "            m[pid]['AvgSentLen'] = np.mean(sent_lens)\n",
    "            text_tokens = nltk.word_tokenize(text)\n",
    "            m[pid]['AvgWordLen'] = np.mean([len(w) for w in text_tokens])\n",
    "            m[pid]['NumAllCaps'] = np.sum([word.isupper() for word in text_tokens])\n",
    "            m[pid]['NumExclams'] = text.count('!')\n",
    "\n",
    "add_sentence_features(meta)\n",
    "\n",
    "plot_feat_hist([(np.asarray([[meta[aid][feat]] for aid in all_answers], dtype=int), feat) for feat in ['AvgSentLen', 'AvgWordLen', 'NumAllCaps', 'NumExclams']],\n",
    "              'feat_hist_AvgSentLen_AvgWordLen_NumAllCaps_NumExclams.png');\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([get_features(aid, ['LinkCount', 'NumCodeLines', 'NumTextTokens', \n",
    "                                   'AvgSentLen', 'AvgWordLen', 'NumAllCaps', \n",
    "                                   'NumExclams']) for aid in all_answers])\n",
    "\n",
    "scores = []\n",
    "from sklearn.utils import shuffle\n",
    "X, Y = shuffle(X, Y_orig, random_state=0)\n",
    "cv = KFold(n=len(X), n_folds=N_FOLDS)\n",
    "\n",
    "for train, test in tqdm(cv, total=N_FOLDS):    \n",
    "    clf = KNeighborsClassifier()\n",
    "    clf.fit(X[train], Y[train])\n",
    "    scores.append(clf.score(X[test], Y[test]))\n",
    "\n",
    "print(\"Mean(scores)=%.5f\\tStddev(scores)=%.5f\"%(np.mean(scores), np.std(scores))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High or low bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc, classification_report\n",
    "\n",
    "def plot_bias_variance(data_sizes, train_errors, test_errors, name, title):\n",
    "    plt.figure(num=None, figsize=(6, 5), dpi=300)\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('Data set size')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title(\"Bias-Variance for '%s'\" % name)\n",
    "    plt.plot(\n",
    "        data_sizes, test_errors, \"--\", data_sizes, train_errors, \"b-\", lw=1)\n",
    "    plt.legend([\"test error\", \"train error\"], loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "\n",
    "def plot_pr(auc_score, name, precision, recall, label=None):\n",
    "    plt.figure(num=None, figsize=(6, 5))\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('P/R (AUC=%0.2f) / %s' % (auc_score, label))\n",
    "    plt.fill_between(recall, precision, alpha=0.5)\n",
    "    plt.grid(True)\n",
    "    plt.plot(recall, precision, lw=1)\n",
    "    filename = name.replace(\" \", \"_\")\n",
    "    plt.savefig(os.path.join(CHART_DIR, \"pr_\" + filename + \".png\"))\n",
    "\n",
    "def plot_feat_importance(feature_names, clf, name):\n",
    "    plt.figure(num=None, figsize=(6, 5))\n",
    "    coef_ = clf.coef_\n",
    "    important = np.argsort(np.absolute(coef_.ravel()))\n",
    "    f_imp = feature_names[important]\n",
    "    coef = coef_.ravel()[important]\n",
    "    inds = np.argsort(coef)\n",
    "    f_imp = f_imp[inds]\n",
    "    coef = coef[inds]\n",
    "    xpos = np.array(list(range(len(coef))))\n",
    "    plt.bar(xpos, coef, width=1)\n",
    "\n",
    "    plt.title('Feature importance for %s' % (name))\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(np.arange(len(coef)))\n",
    "    labels = ax.set_xticklabels(f_imp)\n",
    "    for label in labels:\n",
    "        label.set_rotation(90)\n",
    "    filename = name.replace(\" \", \"_\")\n",
    "    plt.savefig(os.path.join(\n",
    "        CHART_DIR, \"feat_imp_%s.png\" % filename), bbox_inches=\"tight\")\n",
    "\n",
    "def measure(clf_class, parameters, name, X, Y, data_size=None, plot=False, classifying_answer='good', feature_names=None):\n",
    "    if data_size is not None:\n",
    "        X = X[:data_size]\n",
    "        Y = Y[:data_size]\n",
    "\n",
    "    cv = KFold(n=len(X), n_folds=10)\n",
    "\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    scores = []\n",
    "    roc_scores = []\n",
    "    fprs, tprs = [], []\n",
    "\n",
    "    pr_scores = []\n",
    "    precisions, recalls, thresholds = [], [], []\n",
    "\n",
    "    for fold_idx, (train, test) in enumerate(cv):\n",
    "        X_train, y_train = X[train], Y[train]\n",
    "        X_test, y_test = X[test], Y[test]\n",
    "\n",
    "        only_one_class_in_train = len(set(y_train)) == 1\n",
    "        only_one_class_in_test = len(set(y_test)) == 1\n",
    "        if only_one_class_in_train or only_one_class_in_test:\n",
    "            # this would pose problems later on\n",
    "            continue\n",
    "\n",
    "        clf = clf_class(**parameters)\n",
    "\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        train_score = clf.score(X_train, y_train)\n",
    "        test_score = clf.score(X_test, y_test)\n",
    "\n",
    "        train_errors.append(1 - train_score)\n",
    "        test_errors.append(1 - test_score)\n",
    "\n",
    "        scores.append(test_score)\n",
    "        proba = clf.predict_proba(X_test)\n",
    "\n",
    "        label_idx = 1\n",
    "        fpr, tpr, roc_thresholds = roc_curve(y_test, proba[:, label_idx])\n",
    "        precision, recall, pr_thresholds = precision_recall_curve(y_test, proba[:, label_idx])\n",
    "\n",
    "        roc_scores.append(auc(fpr, tpr))\n",
    "        fprs.append(fpr)\n",
    "        tprs.append(tpr)\n",
    "\n",
    "        pr_scores.append(auc(recall, precision))\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        thresholds.append(pr_thresholds)\n",
    "\n",
    "        # This threshold is determined at the end of the chapter,\n",
    "        # where we find conditions such that precision is in the area of\n",
    "        # about 80%. With it we trade off recall for precision.\n",
    "        threshold_for_detecting_good_answers = 0.59\n",
    "\n",
    "        if False:\n",
    "            print(\"Clone #%i\" % fold_idx)\n",
    "            print(classification_report(y_test, proba[:, label_idx] >\n",
    "                  threshold_for_detecting_good_answers, target_names=['not accepted', 'accepted']))\n",
    "\n",
    "    # get medium clone\n",
    "    scores_to_sort = pr_scores  # roc_scores\n",
    "    medium = np.argsort(scores_to_sort)[len(scores_to_sort) // 2]\n",
    "    # print(\"Medium clone is #%i\" % medium)\n",
    "\n",
    "    if plot:\n",
    "        #plot_roc(roc_scores[medium], name, fprs[medium], tprs[medium])\n",
    "        plot_pr(pr_scores[medium], name, precisions[medium],\n",
    "                recalls[medium], classifying_answer + \" answers\")\n",
    "\n",
    "        if hasattr(clf, 'coef_'):\n",
    "            plot_feat_importance(feature_names, clf, name)\n",
    "\n",
    "    summary = (name,\n",
    "               np.mean(scores), np.std(scores),\n",
    "               np.mean(roc_scores), np.std(roc_scores),\n",
    "               np.mean(pr_scores), np.std(pr_scores))\n",
    "        \n",
    "    precisions = precisions[medium]\n",
    "    recalls = recalls[medium]\n",
    "    thresholds = np.hstack(([0], thresholds[medium]))\n",
    "    idx80 = precisions >= 0.8\n",
    "    # print(\"P=%.2f R=%.2f thresh=%.2f\" % (precisions[idx80][0], recalls[idx80][0], thresholds[idx80][0]))\n",
    "\n",
    "    return np.mean(train_errors), np.mean(test_errors), summary\n",
    "\n",
    "def bias_variance_analysis(clf_class, parameters, name, X, Y):\n",
    "    data_sizes = np.arange(60, 2000, 4)\n",
    "\n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for data_size in data_sizes:\n",
    "        train_error, test_error, summary = measure(clf_class, parameters, name, X, Y, data_size=data_size)\n",
    "        train_errors.append(train_error)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "    plot_bias_variance(data_sizes, train_errors,\n",
    "                       test_errors, name, \"Bias-Variance for '%s'\" % name)\n",
    "\n",
    "bias_variance_analysis(KNeighborsClassifier, {'n_neighbors': 5}, \"5NN\", X, Y)\n",
    "plt.savefig(os.path.join(CHART_DIR, \"bv_5NN_all.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe simplifying the feature space helps. Let's try out to use only `LinkCount` and `NumTextTokens`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_simp = np.asarray([get_features(aid, ['LinkCount', 'NumTextTokens']) for aid in all_answers])\n",
    "X_simp, Y_simp = shuffle(X_simp, Y_orig, random_state=0)\n",
    "\n",
    "bias_variance_analysis(KNeighborsClassifier, {'n_neighbors': 5}, \"5NN\", X_simp, Y_simp)\n",
    "plt.savefig(os.path.join(CHART_DIR, \"bv_5NN_simp.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it help to reduce the model complexity by increasing $k$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('k\\tmean(scores)\\tstddev(scores)')\n",
    "for k in [5, 10, 40]:\n",
    "    train_error, test_error, summary = measure(KNeighborsClassifier, {'n_neighbors': k}, \"%iNN\" % k, X, Y)\n",
    "    print('%d\\t%.4f\\t\\t%.4f'%(k,summary[1], summary[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helps a bit, but do we really want to compare with 40 different samples each time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_k_complexity(ks, train_errors, test_errors):\n",
    "    plt.figure(num=None, figsize=(6, 5), dpi=300)\n",
    "    plt.ylim([0.0, 1.0])\n",
    "    plt.xlabel('$k$')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Errors for different values of $k$')\n",
    "    plt.plot(ks, test_errors, \"--\", ks, train_errors, \"-\", lw=1)\n",
    "    plt.legend([\"test error\", \"train error\"], loc=\"upper right\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(CHART_DIR, \"kcomplexity.png\"), bbox_inches=\"tight\")\n",
    "\n",
    "def k_complexity_analysis(clf_class, X, Y):\n",
    "    # Measure for different k's: [1,2,..,20,25,..,100]\n",
    "    ks = np.hstack((np.arange(1, 21), np.arange(25, 101, 5)))\n",
    "    \n",
    "    train_errors = []\n",
    "    test_errors = []\n",
    "\n",
    "    for k in ks:\n",
    "        train_error, test_error, summary = measure(clf_class, {'n_neighbors': k}, \"%dNN\" % k, X, Y, data_size=2000)\n",
    "        train_errors.append(train_error)\n",
    "        test_errors.append(test_error)\n",
    "\n",
    "    plot_k_complexity(ks, train_errors, test_errors)\n",
    "\n",
    "k_complexity_analysis(KNeighborsClassifier, X, Y)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we won't get much better with increasing values of $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using logistic regression\n",
    "Creating some toy data to visualize how logistic regression works..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "np.random.seed(3)\n",
    "\n",
    "NUM_PER_CLASS = 40\n",
    "X_log = np.hstack((norm.rvs(2, size=NUM_PER_CLASS, scale=2),\n",
    "              norm.rvs(8, size=NUM_PER_CLASS, scale=3)))\n",
    "y_log = np.hstack((np.zeros(NUM_PER_CLASS),\n",
    "               np.ones(NUM_PER_CLASS))).astype(int)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 4), dpi=300)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.xlim((-5, 20))\n",
    "plt.scatter(X_log, y_log, c=np.array(['blue', 'red'])[y_log], s=10)\n",
    "plt.xlabel(\"feature value\")\n",
    "plt.ylabel(\"class\")\n",
    "    \n",
    "plt.savefig(os.path.join(CHART_DIR, \"log_reg_example_data.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model(clf, X):\n",
    "    '''\n",
    "    https://en.wikipedia.org/wiki/Logistic_regression\n",
    "    '''\n",
    "    return 1.0 / (1.0 + np.exp(-(clf.intercept_ + clf.coef_ * X)))\n",
    "\n",
    "X_test = np.arange(-5, 20, 0.1)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logclf = LogisticRegression()\n",
    "logclf.fit(X_log.reshape(NUM_PER_CLASS * 2, 1), y_log)\n",
    "print(np.exp(logclf.intercept_), np.exp(logclf.coef_.ravel()))\n",
    "print(\"P(x=-1)=%.2f\\tP(x=7)=%.2f\" %(lr_model(logclf, -1), lr_model(logclf, 7)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 4), dpi=300)\n",
    "plt.xlim((-5, 20))\n",
    "plt.scatter(X_log, y_log, c=np.array(['blue', 'red'])[y], s=5)\n",
    "plt.plot(X_test, lr_model(logclf, X_test).ravel(), c='green')\n",
    "plt.plot(X_test, np.ones(X_test.shape[0]) * 0.5, \"--\")\n",
    "plt.xlabel(\"feature value\")\n",
    "plt.ylabel(\"class\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(CHART_DIR, \"log_reg_example_fitted.png\"), bbox_inches=\"tight\")\n",
    "\n",
    "X_odds = np.arange(0.001, 1, 0.001)\n",
    "plt.figure(figsize=(10, 4), dpi=300)\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.xlim((0, 1))\n",
    "plt.ylim((0, 10))\n",
    "plt.plot(X_odds, X_odds / (1 - X_odds))\n",
    "plt.xlabel(\"P\")\n",
    "plt.ylabel(\"odds = P / (1-P)\")\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.xlim((0, 1))\n",
    "plt.plot(X_odds, np.log(X_odds / (1 - X_odds)))\n",
    "plt.xlabel(\"P\")\n",
    "plt.ylabel(\"log(odds) = log(P / (1-P))\")\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(CHART_DIR, \"log_reg_log_odds.png\"), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('C\\tmean(scores)\\tstddev(scores)') \n",
    "for C in [0.01, 0.1, 1.0, 10.0]:\n",
    "    name = \"LogReg C=%.2f\" % C\n",
    "    train_error, test_error, summary = measure(LogisticRegression, {'penalty': 'l2', 'C': C}, name, X, Y)\n",
    "\n",
    "    print('%.2f\\t%.4f\\t\\t%.4f'%(C,summary[1], summary[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array((\n",
    "    'NumTextTokens',\n",
    "    'NumCodeLines',\n",
    "    'LinkCount',\n",
    "    'AvgSentLen',\n",
    "    'AvgWordLen',\n",
    "    'NumAllCaps',\n",
    "    'NumExclams',\n",
    "    'NumImages'\n",
    "))\n",
    "C = 0.01\n",
    "classifying_answer = \"good\"\n",
    "#classifying_answer = \"poor\"\n",
    "\n",
    "X_orig = np.asarray([get_features(aid, ['LinkCount', 'NumCodeLines', 'NumTextTokens', \n",
    "                                   'AvgSentLen', 'AvgWordLen', 'NumAllCaps', \n",
    "                                   'NumExclams']) for aid in all_answers])\n",
    "\n",
    "Y_orig_good = np.asarray([meta[aid]['Score'] > 0 for aid in all_answers])\n",
    "Y_orig_poor = np.asarray([meta[aid]['Score'] <= 0 for aid in all_answers])\n",
    "\n",
    "X_new, Y_good, Y_poor = shuffle(X_orig, Y_orig_good, Y_orig_poor)\n",
    "    \n",
    "name = \"LogReg C=%.2f\" % C\n",
    "measure(LogisticRegression, {'penalty': 'l2', 'C': C}, name, X_new, Y_good, plot=True, classifying_answer='good', feature_names=feature_names)\n",
    "measure(LogisticRegression, {'penalty': 'l2', 'C': C}, name, X_new, Y_poor, plot=True, classifying_answer='poor', feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old=json.load(open(r\"C:\\repo\\ML_Book\\BuildingMachineLearningSystemsWithPython\\ch04_3rd\\data\\old\\chosen-meta.json\"))\n",
    "new=json.load(open(r\"C:\\repo\\ML_Book\\BuildingMachineLearningSystemsWithPython\\ch04_3rd\\data\\chosen-meta.json\"))\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o,n = set(old.keys()), set(new.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(o-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(o-n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
