{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Machine Learning Systems with Python - Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is supporting material for the book `Building Machine Learning Systems with Python` by [Willi Richert](https://www.linkedin.com/in/willirichert/) and [Luis Pedro Coelho](https://www.linkedin.com/in/luispedrocoelho/)  published by PACKT Publishing.\n",
    "\n",
    "It is made available under the MIT License.\n",
    "\n",
    "All code examples use Python in version..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.6.3 |Anaconda custom (64-bit)| (default, Nov  8 2017, 15:10:56) [MSC v.1900 64 bit (AMD64)]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Use all screen real-estate\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading the data\n",
    "In this chapter we will use the StackOverflow data from https://archive.org/download/stackexchange (while downloading, you have a couple hours time to contemplate whether now would be a good time to donate to the awesome archive.org :-) )\n",
    "Since it is updated on a regular basis, you might get slightly different numbers. In this chapter we use this version:\n",
    "```\n",
    "stackoverflow.com-Posts.7z                        08-Dec-2017 22:31     11.3G\n",
    "```\n",
    "After downloading it, you need to unzip it with [7-Zip](http://www.7-zip.de/download.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting and filtering it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from xml F:\\Stack Exchange Data Dump - Dec 2017\\stackoverflow.com-Posts\\posts-2012.xml\n",
      "Filtered: F:\\Stack Exchange Data Dump - Dec 2017\\stackoverflow.com-Posts\\filtered.tsv\n",
      "Meta: F:\\Stack Exchange Data Dump - Dec 2017\\stackoverflow.com-Posts\\filtered-meta.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "# TODO change before merging to master\n",
    "#DATA_DIR = \"data\"  # put your posts-2012.xml into this directory\n",
    "DATA_DIR = r'F:\\Stack Exchange Data Dump - Dec 2017\\stackoverflow.com-Posts'\n",
    "CHART_DIR = \"charts\"\n",
    "if not os.path.exists(CHART_DIR):\n",
    "    os.mkdir(CHART_DIR)\n",
    "\n",
    "fn_posts_all = os.path.join(DATA_DIR, \"posts.xml\")\n",
    "fn_posts = os.path.join(DATA_DIR, \"posts-2012.xml\")\n",
    "print(\"Reading from xml %s\" % fn_posts)\n",
    "\n",
    "fn_filtered = os.path.join(DATA_DIR, \"filtered.tsv\")\n",
    "print(\"Filtered: %s\" % fn_filtered)\n",
    "fn_filtered_meta = os.path.join(DATA_DIR, \"filtered-meta.json\")\n",
    "print(\"Meta: %s\" % fn_filtered_meta)\n",
    "\n",
    "fn_chosen = os.path.join(DATA_DIR, \"chosen.tsv\")\n",
    "fn_chosen_meta = os.path.join(DATA_DIR, \"chosen-meta.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 59GB in posts.xml is contain posts from 2008 to 2017. We will use only the posts from the year 2012, which provides enough fun for now. We could simply grep on the command line, but that would take quite a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_match = re.compile(r'^\\s+<row [^>]*CreationDate=\"(\\d+)-')\n",
    "size = os.path.getsize(fn_posts_all)\n",
    "\n",
    "def get_year(line):\n",
    "    m = year_match.match(line)\n",
    "    if m is None:\n",
    "        import pdb;pdb.set_trace()\n",
    "    return int(m.group(1))\n",
    "\n",
    "YEAR = 2012\n",
    "\n",
    "with open(fn_posts_all, 'r', encoding='utf-8') as fa, open(fn_posts, 'w', encoding='utf-8') as f2012:\n",
    "    # first two lines are the xml header and <posts> tag\n",
    "    f2012.write('<?xml version=\"1.0\" encoding=\"utf-8\"?><posts>\\n')    \n",
    "    \n",
    "    right = size//2\n",
    "    delta = right\n",
    "    \n",
    "    # first find some post of YEAR\n",
    "    while True:\n",
    "        fa.seek(right)\n",
    "        fa.readline() # go to next newline\n",
    "        line = fa.readline()\n",
    "        \n",
    "        year = get_year(line)\n",
    "        \n",
    "        delta //= 2\n",
    "        assert delta > 0\n",
    "        \n",
    "        if year>YEAR:\n",
    "            right -= delta\n",
    "        elif year<YEAR:\n",
    "            right += delta\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    # then find where it starts\n",
    "    left = right//2\n",
    "    delta = left\n",
    "    while True:\n",
    "        fa.seek(left)\n",
    "        fa.readline() # go to next newline\n",
    "        line = fa.readline()\n",
    "        \n",
    "        year = get_year(line)\n",
    "        \n",
    "        delta //= 2\n",
    "        if delta == 0:\n",
    "            break\n",
    "        \n",
    "        if year<YEAR:\n",
    "            left += delta\n",
    "            \n",
    "        else:\n",
    "            left, right = left-delta, left\n",
    "    \n",
    "    # and write all posts of that year\n",
    "    while True:\n",
    "        line = fa.readline()\n",
    "        year = get_year(line)\n",
    "        if year == YEAR:\n",
    "            f2012.write(line)\n",
    "        elif year > YEAR:\n",
    "            break\n",
    "        \n",
    "        \n",
    "    # and write the closing tag\n",
    "    f2012.write('</posts>\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdceb86ee6ac4d85a865add508f88e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4511703), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dateutil import parser as dateparser\n",
    "\n",
    "from operator import itemgetter\n",
    "from collections import defaultdict\n",
    "from lxml import etree\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm # we all love nice progress bars, don't we?\n",
    "\n",
    "try:\n",
    "    import ujson as json  # UltraJSON if available\n",
    "except:\n",
    "    print(\"You can also use the normal json module, but you get a XXX speedup if you use ujson instead.\")\n",
    "    raise\n",
    "\n",
    "q_creation = {}  # creation datetimes of questions\n",
    "q_accepted = {}  # id of accepted answer\n",
    "\n",
    "NUM_ROWS = 4511696 # counted by hand\n",
    "\n",
    "meta = {\n",
    "    'question': defaultdict(list), # question -> [(answer Id, IsAccepted, TimeToAnswer, Score), ...]\n",
    "    'total': 0 # questions and answers finally written\n",
    "}\n",
    "\n",
    "# Regular expressions to find code snippets, links, images, and tags, which might help in \n",
    "# designing useful features\n",
    "code_match = re.compile('<pre>(.*?)</pre>', re.MULTILINE | re.DOTALL)\n",
    "link_match = re.compile('<a href=\"http://.*?\".*?>(.*?)</a>', re.MULTILINE | re.DOTALL)\n",
    "img_match = re.compile('<img(.*?)/>', re.MULTILINE | re.DOTALL)\n",
    "tag_match = re.compile('<[^>]*>', re.MULTILINE | re.DOTALL)\n",
    "whitespace_match = re.compile(r'\\s+', re.MULTILINE | re.DOTALL)\n",
    "\n",
    "def extract_features_from_body(s):\n",
    "    '''\n",
    "    This method creates features from the raw post. It already contains all \n",
    "    features that we will use throughout the chapter.\n",
    "    '''\n",
    "    num_code_lines = 0\n",
    "    link_count_in_code = 0\n",
    "    code_free_s = s\n",
    "\n",
    "    num_images = len(img_match.findall(s))\n",
    "\n",
    "    # remove source code and count how many lines\n",
    "    for match_str in code_match.findall(s):\n",
    "        num_code_lines += match_str.count('\\n')\n",
    "        code_free_s = code_match.sub(' ', code_free_s)\n",
    "\n",
    "        # sometimes source code contain links, which we don't want to count\n",
    "        link_count_in_code += len(link_match.findall(match_str))\n",
    "\n",
    "    links = link_match.findall(s)\n",
    "    link_count = len(links) - link_count_in_code\n",
    "\n",
    "    html_free_s = tag_match.sub(' ', code_free_s)\n",
    "    link_free_s = html_free_s\n",
    "    \n",
    "    for link in links:\n",
    "        if link.lower().startswith('http://'):\n",
    "            link_free_s = link_free_s.replace(link, ' ')\n",
    "\n",
    "    whitespace_cleaned_s = whitespace_match.sub(' ', link_free_s)\n",
    "    num_text_tokens = whitespace_cleaned_s.count(' ')\n",
    "\n",
    "    return link_free_s, num_text_tokens, num_code_lines, link_count, num_images\n",
    "\n",
    "years = defaultdict(int)\n",
    "num_questions = 0\n",
    "num_answers = 0\n",
    "\n",
    "def parsexml(fn):\n",
    "    global num_questions, num_answers\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    # etree.iterparse() returns a tuple (event, element). Since we request only\n",
    "    # 'start' events, we pipe the result through an itemgetter that always returns\n",
    "    # the 2nd result.\n",
    "    it = map(itemgetter(1), etree.iterparse(fn, events=('start',)))\n",
    "    \n",
    "    # Get the <posts> element, in which we will parse the <row> elements. While doing so,\n",
    "    # we will need the root handle to clear memory\n",
    "    root = next(it)\n",
    "    \n",
    "    for counter, elem in enumerate(tqdm(it, total=NUM_ROWS)):\n",
    "        \n",
    "        if elem.tag != 'row':\n",
    "            continue\n",
    "            \n",
    "        creation_date = dateparser.parse(elem.get('CreationDate'))\n",
    "        \n",
    "        # to speed up our journey, we restrict the data to posts in 2012\n",
    "        if creation_date.year != 2012:\n",
    "            continue\n",
    "\n",
    "        Id = int(elem.get('Id'))\n",
    "        PostTypeId = int(elem.get('PostTypeId'))\n",
    "        Score = int(elem.get('Score'))\n",
    "\n",
    "        if PostTypeId == 1:\n",
    "            num_questions += 1\n",
    "            years[creation_date.year] += 1\n",
    "\n",
    "            ParentId = -1\n",
    "            TimeToAnswer = 0\n",
    "            q_creation[Id] = creation_date\n",
    "            accepted = elem.get('AcceptedAnswerId')\n",
    "            if accepted:\n",
    "                q_accepted[Id] = int(accepted)\n",
    "            IsAccepted = 0\n",
    "\n",
    "        elif PostTypeId == 2:\n",
    "            num_answers += 1\n",
    "\n",
    "            ParentId = int(elem.get('ParentId'))\n",
    "            if not ParentId in q_creation:\n",
    "                # question was too far in the past\n",
    "                continue\n",
    "\n",
    "            TimeToAnswer = (creation_date - q_creation[ParentId]).seconds\n",
    "\n",
    "            if ParentId in q_accepted:\n",
    "                IsAccepted = int(q_accepted[ParentId] == Id)\n",
    "            else:\n",
    "                IsAccepted = 0\n",
    "\n",
    "            meta['question'][ParentId].append((Id, IsAccepted, TimeToAnswer, Score))\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        Text, NumTextTokens, NumCodeLines, LinkCount, NumImages = extract_features_from_body(elem.get('Body'))\n",
    "\n",
    "        # https://www.ibm.com/developerworks/xml/library/x-hiperfparse/\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "            \n",
    "        values = (Id, ParentId,\n",
    "                  IsAccepted,\n",
    "                  TimeToAnswer, Score,\n",
    "                  Text.encode(\"utf-8\"),\n",
    "                  NumTextTokens, NumCodeLines, LinkCount, NumImages)\n",
    "\n",
    "        yield values\n",
    "\n",
    "\n",
    "    print(\"Found %i posts\" % counter)\n",
    "\n",
    "if any(not os.path.exists(fn) for fn in [fn_filtered, fn_filtered_meta]):\n",
    "    total = 0\n",
    "    with open(fn_filtered, \"w\") as f:\n",
    "        for values in parsexml(fn_posts):\n",
    "            line = \"\\t\".join(map(str, values))\n",
    "            f.write(line + \"\\n\")\n",
    "            total += 1\n",
    "    meta['total'] = total\n",
    "                \n",
    "    with open(fn_filtered_meta, \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "    \n",
    "    print(\"years:\", years)\n",
    "    print(\"#qestions: %i\" % num_questions)\n",
    "    print(\"#answers: %i\" % num_answers)\n",
    "    \n",
    "else:\n",
    "    print(\"Skipping the conversion step, loading data from %s ...\" % fn_filtered_meta)\n",
    "    filtered_meta = json.load(open(fn_filtered_meta, \"r\"))\n",
    "    print(\"... done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to select the answers that we want to keep per question. We do this in two stages:\n",
    " * Stage 1: Select which answers to keep per question (`filter_method` lets you chose among different methods)\n",
    " * Stage 2: Filter the previously stored features according to the answers selected in stage 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8ccd771e1ed4ecd80f18693456b1edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Stage 1:', max=13025863), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "read: 0\n",
      "kept: 0\n"
     ]
    }
   ],
   "source": [
    "# In the book chapter we will use \"negative_positive\".\n",
    "#\n",
    "# \"negative_positive\":   keep the best and worst, but only if we have one with \n",
    "#                        positive and one with negative score\n",
    "# \"only_one_per_class\":  only keep the lowest scoring answer per class in addition to the \n",
    "#                        accepted one\n",
    "# \"sample_per_question\": if not None, specifies the number of unaccepted per question\n",
    "# \"half-half\":           equal share of questions that are unanswered and those that are \n",
    "#                        answered\n",
    "\n",
    "filter_method = \"negative_positive\"\n",
    "\n",
    "MAX_ANSWERS_PER_QUESTIONS = 10  # used by filter_method \"sample_per_question\"\n",
    "\n",
    "NUM_QUESTION_SAMPLE = 0\n",
    "\n",
    "posts_to_keep = set()\n",
    "found_questions = 0\n",
    "\n",
    "unaccepted_scores = {}\n",
    "\n",
    "has_q_accepted_a = {}\n",
    "num_q_with_accepted_a = 0\n",
    "num_q_without_accepted_a = 0\n",
    "\n",
    "question = filtered_meta['question']\n",
    "for ParentId, posts in tqdm(question.items(), total=len(question), desc=\"Stage 1:\"):\n",
    "    assert ParentId != -1\n",
    "\n",
    "    if len(posts) < 2:\n",
    "        continue\n",
    "\n",
    "    ParentId = int(ParentId)\n",
    "    AllIds = set([ParentId])\n",
    "    AcceptedId = None\n",
    "    UnacceptedId = None\n",
    "    UnacceptedIds = []\n",
    "    UnacceptedScore = sys.maxsize\n",
    "\n",
    "    NegativeScoreIds = []\n",
    "    PositiveScoreIds = []\n",
    "\n",
    "    if filter_method == \"half-half\":\n",
    "\n",
    "        has_accepted_a = False\n",
    "        for post in posts:\n",
    "            Id, IsAccepted, TimeToAnswer, Score = post\n",
    "\n",
    "            if IsAccepted:\n",
    "                has_accepted_a = True\n",
    "                break\n",
    "\n",
    "        has_q_accepted_a[ParentId] = has_accepted_a\n",
    "\n",
    "        if has_accepted_a:\n",
    "            if num_q_with_accepted_a < NUM_QUESTION_SAMPLE / 2:\n",
    "                num_q_with_accepted_a += 1\n",
    "                posts_to_keep.add(ParentId)\n",
    "        else:\n",
    "            if num_q_without_accepted_a < NUM_QUESTION_SAMPLE / 2:\n",
    "                num_q_without_accepted_a += 1\n",
    "                posts_to_keep.add(ParentId)\n",
    "\n",
    "        if num_q_without_accepted_a + num_q_with_accepted_a > NUM_QUESTION_SAMPLE:\n",
    "            assert -1 not in posts_to_keep\n",
    "            break\n",
    "\n",
    "    else:\n",
    "\n",
    "        for post in posts:\n",
    "            Id, IsAccepted, TimeToAnswer, Score = post\n",
    "\n",
    "            if filter_method == \"all\":\n",
    "                AllIds.add(int(Id))\n",
    "\n",
    "            elif filter_method == \"only_one_per_class\":\n",
    "                if IsAccepted:\n",
    "                    AcceptedId = Id\n",
    "                elif Score < UnacceptedScore:\n",
    "                    UnacceptedScore = Score\n",
    "                    UnacceptedId = Id\n",
    "\n",
    "            elif filter_method == \"sample_per_question\":\n",
    "                if IsAccepted:\n",
    "                    AcceptedId = Id\n",
    "                else:\n",
    "                    UnacceptedIds.append(Id)\n",
    "\n",
    "            elif filter_method == \"negative_positive\":\n",
    "                if Score <= 0:\n",
    "                    NegativeScoreIds.append((Score, Id))\n",
    "                elif Score > 0:\n",
    "                    PositiveScoreIds.append((Score, Id))\n",
    "\n",
    "            else:\n",
    "                raise ValueError(filter_method)\n",
    "\n",
    "        added = False\n",
    "        if filter_method == \"all\":\n",
    "            posts_to_keep.update(AllIds)\n",
    "            added = True\n",
    "            \n",
    "        elif filter_method == \"only_one_per_class\":\n",
    "            if AcceptedId is not None and UnacceptedId is not None:\n",
    "                posts_to_keep.add(ParentId)\n",
    "                posts_to_keep.add(AcceptedId)\n",
    "                posts_to_keep.add(UnacceptedId)\n",
    "                added = True\n",
    "\n",
    "        elif filter_method == \"sample_per_question\":\n",
    "            if AcceptedId is not None and UnacceptedIds is not None:\n",
    "                posts_to_keep.add(ParentId)\n",
    "                posts_to_keep.add(AcceptedId)\n",
    "                posts_to_keep.update(UnacceptedIds[:MAX_ANSWERS_PER_QUESTIONS])\n",
    "                added = True\n",
    "\n",
    "        elif filter_method == \"negative_positive\":\n",
    "            if PositiveScoreIds and NegativeScoreIds:\n",
    "                posts_to_keep.add(ParentId)\n",
    "\n",
    "                posScore, posId = sorted(PositiveScoreIds)[-1]\n",
    "                posts_to_keep.add(posId)\n",
    "\n",
    "                negScore, negId = sorted(NegativeScoreIds)[0]\n",
    "                posts_to_keep.add(negId)\n",
    "                \n",
    "                added = True\n",
    "\n",
    "        if added:\n",
    "            found_questions += 1\n",
    "\n",
    "    if NUM_QUESTION_SAMPLE and found_questions >= NUM_QUESTION_SAMPLE:\n",
    "        print(\"Using only a sample of %i questions\" % NUM_QUESTION_SAMPLE)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "already_written = set()\n",
    "chosen_meta = defaultdict(dict)\n",
    "\n",
    "with open(fn_chosen, \"w\") as f:\n",
    "    for line in data(filtered):\n",
    "        strId, ParentId, IsAccepted, TimeToAnswer, Score, Text, NumTextTokens, NumCodeLines, LinkCount, NumImages = line\n",
    "        Text = Text.strip()\n",
    "\n",
    "        total += 1\n",
    "\n",
    "        Id = int(strId)\n",
    "        if Id in posts_to_keep:\n",
    "            if Id in already_written:\n",
    "                print(Id, \"is already written\")\n",
    "                continue\n",
    "\n",
    "            if kept % 100 == 0:\n",
    "                print(kept)\n",
    "\n",
    "            # setting meta info\n",
    "            post = chosen_meta_dict[Id]\n",
    "            post['ParentId'] = int(ParentId)\n",
    "            post['IsAccepted'] = int(IsAccepted)\n",
    "            post['TimeToAnswer'] = int(TimeToAnswer)\n",
    "            post['Score'] = int(Score)\n",
    "            post['NumTextTokens'] = int(NumTextTokens)\n",
    "            post['NumCodeLines'] = int(NumCodeLines)\n",
    "            post['LinkCount'] = int(LinkCount)\n",
    "            post['MisSpelledFraction'] = misspelled_fraction(Text)\n",
    "            post['NumImages'] = int(NumImages)\n",
    "            post['idx'] = kept  # index into the file\n",
    "\n",
    "            if int(ParentId) == -1:\n",
    "                q = chosen_meta_dict[Id]\n",
    "\n",
    "                if not 'Answers' in q:\n",
    "                    q['Answers'] = []\n",
    "\n",
    "                if filter_method == \"half-half\":\n",
    "                    q['HasAcceptedAnswer'] = has_q_accepted_a[Id]\n",
    "\n",
    "            else:\n",
    "                q = chosen_meta_dict[int(ParentId)]\n",
    "\n",
    "                if int(IsAccepted) == 1:\n",
    "                    assert 'HasAcceptedAnswer' not in q\n",
    "                    q['HasAcceptedAnswer'] = True\n",
    "\n",
    "                if 'Answers' not in q:\n",
    "                    q['Answers'] = [Id]\n",
    "                else:\n",
    "                    q['Answers'].append(Id)\n",
    "\n",
    "            f.writelines(\"%s\\t%s\\n\" % (Id, Text))\n",
    "            kept += 1\n",
    "\n",
    "with open(fn_chosen_meta, \"w\") as fm:\n",
    "    json.dump(chosen_meta, fm)\n",
    "\n",
    "print(\"read:\", total)\n",
    "print(\"kept:\", kept)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility functions to access the sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_meta(filename):\n",
    "    meta = json.load(open(filename, \"r\"))\n",
    "    keys = list(meta.keys())\n",
    "\n",
    "    # JSON only allows string keys, changing that to int\n",
    "    for key in keys:\n",
    "        meta[int(key)] = meta[key]\n",
    "        del meta[key]\n",
    "\n",
    "    # map post Id to index in vectorized\n",
    "    id_to_idx = {}\n",
    "    # and back\n",
    "    idx_to_id = {}\n",
    "\n",
    "    for PostId, Info in meta.items():\n",
    "        id_to_idx[PostId] = idx = Info['idx']\n",
    "        idx_to_id[idx] = PostId\n",
    "\n",
    "    return meta, id_to_idx, idx_to_id\n",
    "\n",
    "meta, id_to_idx, idx_to_id = load_meta(fn_chosen_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the features and labeling them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_questions = sorted([q for q, v in meta.items() if v['ParentId'] == -1])\n",
    "all_answers = sorted([q for q, v in meta.items() if v['ParentId'] != -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An answer is labeled as positive if it has a score greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_orig = np.asarray([meta[aid]['Score'] > 0 for aid in all_answers])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating our first classifier: kNN using only LinkCount as a feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=2, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import neighbors \n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=2) \n",
    "print(knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy training data: map {1,2,3} to 0 and {4,5,6} to 1\n",
    "knn.fit([[1],[2],[3],[4],[5],[6]], [0,0,0,1,1,1])\n",
    "knn.predict(1.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict(37) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict_proba(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict_proba(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict_proba(3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model using only LinkCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how good is `LinkCount`? Let's look at its histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "X = np.asarray([[meta[aid]['LinkCount']] for aid in all_answers])\n",
    "\n",
    "plt.figure(figsize=(5,4), dpi=300) # width and height of the plot in inches\n",
    "\n",
    "plt.title('LinkCount')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Occurrence')\n",
    "\n",
    "n, bins, patches = plt.hist(X, bins=50, normed=True, alpha=0.75)\n",
    "plt.xlim(0, 50)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.savefig(os.path.join(CHART_DIR, 'feat_hist_linkcount.png'), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so most posts don't contain a link at all, but let's try nevertheless..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on LinkCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "scores = []\n",
    "N_FOLDS = 5\n",
    "from sklearn.utils import shuffle\n",
    "X, Y = shuffle(X, Y_orig, random_state=0)\n",
    "cv = KFold(n=len(X), n_folds=N_FOLDS)\n",
    "\n",
    "for train, test in tqdm(cv, total=N_FOLDS):    \n",
    "    clf = neighbors.KNeighborsClassifier()\n",
    "    clf.fit(X[train], Y[train])\n",
    "    scores.append(clf.score(X[test], Y[test]))\n",
    "\n",
    "print(\"Mean(scores)=%.5f\\tStddev(scores)=%.5f\"%(np.mean(scores), np.std(scores))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feat_hist(data_name_list, filename=None):\n",
    "    if len(data_name_list) > 1:\n",
    "        assert filename is not None\n",
    "\n",
    "    num_rows = int(1 + (len(data_name_list) - 1) / 2)\n",
    "    num_cols = int(1 if len(data_name_list) == 1 else 2)\n",
    "    pylab.figure(figsize=(5 * num_cols, 4 * num_rows), dpi=300)\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            pylab.subplot(num_rows, num_cols, 1 + i * num_cols + j)\n",
    "            x, name = data_name_list[i * num_cols + j]\n",
    "            pylab.title(name)\n",
    "            pylab.xlabel('Value')\n",
    "            pylab.ylabel('Occurrence')\n",
    "            # the histogram of the data\n",
    "            max_val = np.max(x)\n",
    "            if max_val <= 1.0:\n",
    "                bins = 50\n",
    "            elif max_val > 50:\n",
    "                bins = 50\n",
    "            else:\n",
    "                bins = max_val\n",
    "            n, bins, patches = pylab.hist(\n",
    "                x, bins=bins, normed=1, alpha=0.75)\n",
    "\n",
    "            pylab.grid(True)\n",
    "\n",
    "    if not filename:\n",
    "        filename = \"feat_hist_%s.png\" % name.replace(\" \", \"_\")\n",
    "\n",
    "    pylab.savefig(os.path.join(CHART_DIR, filename), bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "plot_feat_hist([(np.asarray([[meta[aid]['NumCodeLines']] for aid in all_answers]), 'NumCodeLines'),\n",
    "                (np.asarray([[meta[aid]['NumTextTokens']] for aid in all_answers]), 'NumTextTokens')],\n",
    "              'feat_hist_CodeLinkes_TextTokens.png');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(aid, feature_names):\n",
    "    return tuple(meta[aid][fn] for fn in feature_names)\n",
    "\n",
    "X = np.asarray([get_features(aid, ['LinkCount', 'NumCodeLines', 'NumTextTokens']) for aid in all_answers])\n",
    "\n",
    "scores = []\n",
    "N_FOLDS = 5\n",
    "from sklearn.utils import shuffle\n",
    "X, Y = shuffle(X, Y_orig, random_state=0)\n",
    "cv = KFold(n=len(X), n_folds=N_FOLDS)\n",
    "\n",
    "for train, test in tqdm(cv, total=N_FOLDS):    \n",
    "    clf = neighbors.KNeighborsClassifier()\n",
    "    clf.fit(X[train], Y[train])\n",
    "    scores.append(clf.score(X[test], Y[test]))\n",
    "\n",
    "print(\"Mean(scores)=%.5f\\tStddev(scores)=%.5f\"%(np.mean(scores), np.std(scores))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If more features are good, even more features should be even better\n",
    "Let's create some more text based features like average sentence and word length, how many words are CAPITALIZED or contain exclamation marks.\n",
    "\n",
    "We simply at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.asarray([get_features(aid, ['LinkCount', 'NumCodeLines', 'NumTextTokens', \n",
    "                                   'AvgSentLen', 'AvgWordLen', 'NumAllCaps', \n",
    "                                   'NumExclams',]) for aid in all_answers])\n",
    "\n",
    "scores = []\n",
    "N_FOLDS = 5\n",
    "from sklearn.utils import shuffle\n",
    "X, Y = shuffle(X, Y_orig, random_state=0)\n",
    "cv = KFold(n=len(X), n_folds=N_FOLDS)\n",
    "\n",
    "for train, test in tqdm(cv, total=N_FOLDS):    \n",
    "    clf = neighbors.KNeighborsClassifier()\n",
    "    clf.fit(X[train], Y[train])\n",
    "    scores.append(clf.score(X[test], Y[test]))\n",
    "\n",
    "print(\"Mean(scores)=%.5f\\tStddev(scores)=%.5f\"%(np.mean(scores), np.std(scores))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "feature_names = np.array((\n",
    "    'NumTextTokens',\n",
    "    'NumCodeLines',\n",
    "    'LinkCount',\n",
    "    'AvgSentLen',\n",
    "    'AvgWordLen',\n",
    "    'NumAllCaps',\n",
    "    'NumExclams',\n",
    "    'NumImages'\n",
    "))\n",
    "\n",
    "def fetch_posts(fn, with_index=True, line_count=-1):\n",
    "    count = 0\n",
    "\n",
    "    for line in open(fn, \"r\"):\n",
    "        count += 1\n",
    "        if line_count > 0 and count > line_count:\n",
    "            break\n",
    "\n",
    "        Id, Text = line.split(\"\\t\")\n",
    "        Text = Text.strip()\n",
    "\n",
    "        if with_index:\n",
    "            yield int(Id), Text\n",
    "\n",
    "        else:\n",
    "            yield Text\n",
    "            \n",
    "def prepare_sent_features():\n",
    "    for pid, text in fetch_posts(chosen, with_index=True):\n",
    "        if not text:\n",
    "            meta[pid]['AvgSentLen'] = meta[pid]['AvgWordLen'] = 0\n",
    "        else:\n",
    "            from platform import python_version\n",
    "            if python_version().startswith('2'):\n",
    "                text = text.decode('utf-8')\n",
    "            sent_lens = [len(nltk.word_tokenize(\n",
    "                sent)) for sent in nltk.sent_tokenize(text)]\n",
    "            meta[pid]['AvgSentLen'] = np.mean(sent_lens)\n",
    "            meta[pid]['AvgWordLen'] = np.mean(\n",
    "                [len(w) for w in nltk.word_tokenize(text)])\n",
    "\n",
    "        meta[pid]['NumAllCaps'] = np.sum(\n",
    "            [word.isupper() for word in nltk.word_tokenize(text)])\n",
    "\n",
    "        meta[pid]['NumExclams'] = text.count('!')\n",
    "\n",
    "\n",
    "prepare_sent_features()\n",
    "\n",
    "qa_X = np.asarray([get_features(aid) for aid in all_answers])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feat_hist(data_name_list):\n",
    "    '''\n",
    "    Plots feature histograms for all features specified in data_name_list\n",
    "    '''\n",
    "    pylab.figure(num=None, figsize=(8, 6))\n",
    "    num_rows = int(1 + (len(data_name_list) - 1) / 2)\n",
    "    num_cols = int(1 if len(data_name_list) == 1 else 2)\n",
    "    pylab.figure(figsize=(5 * num_cols, 4 * num_rows))\n",
    "\n",
    "    for i in range(num_rows):\n",
    "        for j in range(num_cols):\n",
    "            pylab.subplot(num_rows, num_cols, 1 + i * num_cols + j)\n",
    "            x, name = data_name_list[i * num_cols + j]\n",
    "            pylab.title(name)\n",
    "            pylab.xlabel('Value')\n",
    "            pylab.ylabel('Fraction')\n",
    "            # the histogram of the data\n",
    "            max_val = np.max(x)\n",
    "            if max_val <= 1.0:\n",
    "                bins = 50\n",
    "            elif max_val > 50:\n",
    "                bins = 50\n",
    "            else:\n",
    "                bins = max_val\n",
    "            n, bins, patches = pylab.hist(\n",
    "                x, bins=bins, normed=1, alpha=0.75)\n",
    "\n",
    "            pylab.grid(True)\n",
    "\n",
    "    if len(data_name_list) == 1:\n",
    "        filename = \"feat_hist_%s.png\" % name.replace(\" \", \"_\")\n",
    "    else:\n",
    "        filename = \"featu_hist.png\"\n",
    "\n",
    "    pylab.savefig(os.path.join(CHART_DIR, filename), bbox_inches=\"tight\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
