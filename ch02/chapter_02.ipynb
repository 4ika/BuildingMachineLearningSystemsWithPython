{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2\n",
    "\n",
    "This code is supporting material for the book _Building Machine Learning Systems with Python_ by Willi Richert and Luis Pedro Coelho published by PACKT Publishing\n",
    "\n",
    "It is made available under the MIT License\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Magic command to get inline plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the data with `load_iris` from `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`load_iris` returns an object with several fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = data.data\n",
    "feature_names = data.feature_names\n",
    "target = data.target\n",
    "target_names = data.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use NumPy fancy indexing to get an array of strings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = target_names[target]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot all the 2D projections of the features (since there are only 4 of them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(2, 3)\n",
    "pairs = [(0, 1), (0, 2), (0, 3), (1, 2), (1, 3), (2, 3)]\n",
    "\n",
    "# Set up 3 different pairs of (color, marker)\n",
    "color_markers = [\n",
    "        ('r', '>'),\n",
    "        ('g', 'o'),\n",
    "        ('b', 'x'),\n",
    "        ]\n",
    "for i, (p0, p1) in enumerate(pairs):\n",
    "    ax = axes.flat[i]\n",
    "\n",
    "    for t in range(3):\n",
    "        # Use a different color/marker for each class `t`\n",
    "        c,marker = color_markers[t]\n",
    "        ax.scatter(features[target == t, p0], features[\n",
    "                    target == t, p1], marker=marker, c=c)\n",
    "    ax.set_xlabel(feature_names[p0])\n",
    "    ax.set_ylabel(feature_names[p1])\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning a classification model (a decision tree):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "tr = tree.DecisionTreeClassifier(min_samples_leaf=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`fit` performs the learning (fits the model):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.fit(features, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the decision tree (using an intermediate file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "tree.export_graphviz(tr, feature_names=feature_names, rounded=True, out_file='decision.dot')\n",
    "\n",
    "graphviz.Source(open('decision.dot').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating performance on the training set (which is **not the right way to do it**: see the discussion in the book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prediction = tr.predict(features)\n",
    "print(\"Accuracy: {:.1%}\".format(np.mean(prediction == labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave-one-out crossvalidation (a good way to do it):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in range(len(features)):\n",
    "    train_features = np.delete(features, i, axis=0)\n",
    "    train_labels = np.delete(labels, i, axis=0)\n",
    "    tr.fit(train_features, train_labels)\n",
    "    predictions.append(tr.predict([features[i]]))\n",
    "predictions = np.array(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the LOO predictions with the gold set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy (with LOO CV): {:.1%}\".format(np.mean(predictions.ravel() == labels)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same leave-on-out cross validation with scikit-learn's `model_selection` module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "\n",
    "predictions = model_selection.cross_val_predict(\n",
    "    tr,\n",
    "    features,\n",
    "    labels,\n",
    "    cv=model_selection.LeaveOneOut())\n",
    "print(np.mean(predictions == labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The seeds dataset\n",
    "\n",
    "A slightly more complex dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from load import load_dataset\n",
    "\n",
    "feature_names = [\n",
    "    'area',\n",
    "    'perimeter',\n",
    "    'compactness',\n",
    "    'length of kernel',\n",
    "    'width of kernel',\n",
    "    'asymmetry coefficien',\n",
    "    'length of kernel groove',\n",
    "]\n",
    "data = load_dataset('seeds')\n",
    "features = data['features']\n",
    "target = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now try a _nearest neighbors_ classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything is similar to what we had before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = model_selection.KFold(n_splits=5, shuffle=False)\n",
    "means = []\n",
    "for training,testing in kf.split(features):\n",
    "   # We learn a model for this fold with `fit` and then apply it to the\n",
    "   # testing data with `predict`:\n",
    "   knn.fit(features[training], target[training])\n",
    "   prediction = knn.predict(features[testing])\n",
    "\n",
    "   # np.mean on an array of booleans returns fraction\n",
    " # of correct decisions for this fold:\n",
    "   curmean = np.mean(prediction == target[testing])\n",
    "   means.append(curmean)\n",
    "print('Mean accuracy: {:.1%}'.format(np.mean(means)))\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=5)\n",
    "classifier = Pipeline([('norm', StandardScaler()), ('knn', classifier)])\n",
    "\n",
    "means = []\n",
    "for training,testing in kf.split(features):\n",
    "    # We learn a model for this fold with `fit` and then apply it to the\n",
    "    # testing data with `predict`:\n",
    "    classifier.fit(features[training], target[training])\n",
    "    prediction = classifier.predict(features[testing])\n",
    "\n",
    "    # np.mean on an array of booleans returns fraction\n",
    "    # of correct decisions for this fold:\n",
    "    curmean = np.mean(prediction == target[testing])\n",
    "    means.append(curmean)\n",
    "print('Mean accuracy: {:.1%}'.format(np.mean(means)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_decision_space(clf, features, target, use_color=True):\n",
    "    from matplotlib.colors import ListedColormap\n",
    "\n",
    "    clf.fit(features[:, [0,2]], target)\n",
    "\n",
    "    y0, y1 = features[:, 2].min() * .9, features[:, 2].max() * 1.1\n",
    "    x0, x1 = features[:, 0].min() * .9, features[:, 0].max() * 1.1\n",
    "    X = np.linspace(x0, x1, 1000)\n",
    "    Y = np.linspace(y0, y1, 1000)\n",
    "    X, Y = np.meshgrid(X, Y)\n",
    "    C = clf.predict(np.vstack([X.ravel(), Y.ravel()]).T).reshape(X.shape)\n",
    "    if use_color:\n",
    "        cmap = ListedColormap([(1., .7, .7), (.7, 1., .7), (.7, .7, 1.)])\n",
    "    else:\n",
    "        cmap = ListedColormap([(1., 1., 1.), (.2, .2, .2), (.6, .6, .6)])\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.scatter(features[:, 0], features[:, 2], c=target, cmap=cmap)\n",
    "    for lab, ma in zip(range(3), \"Do^\"):\n",
    "        ax.plot(features[target == lab, 0], features[\n",
    "                 target == lab, 2], ma, c=(1., 1., 1.), ms=6)\n",
    "\n",
    "    ax.set_xlim(x0, x1)\n",
    "    ax.set_ylim(y0, y1)\n",
    "    ax.set_xlabel(feature_names[0])\n",
    "    ax.set_ylabel(feature_names[2])\n",
    "    ax.pcolormesh(X, Y, C, cmap=cmap)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_decision_space(knn, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "classifier = KNeighborsClassifier(n_neighbors=1)\n",
    "classifier = Pipeline([('norm', StandardScaler()),\n",
    "                       ('knn', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_decision_space(classifier, features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forests\n",
    "\n",
    "See the excellent paper [Do we Need Hundreds of Classifiers to Solve Real World Classification Problems?](http://jmlr.org/papers/v15/delgado14a.html) by Delgado et al. (2014) to understand why we recommend random forests as the default classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import ensemble\n",
    "rf = ensemble.RandomForestClassifier(n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, use cross-validation to evaluate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model_selection.cross_val_predict(rf, features, target)\n",
    "print(\"RF accuracy: {:.1%}\".format(np.mean(predict == target)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not in the book, but we can also plot the decision function for a random forest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_= plot_decision_space(rf, features, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
